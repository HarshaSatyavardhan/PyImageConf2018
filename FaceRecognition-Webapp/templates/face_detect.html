<!DOCTYPE html>
<html>
  <title>Face Detection</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  
  
  <body>
    <div class="w3-content" style="max-width:1500px">
      <div class="w3-panel w3-center" >
        <p class="w3-button w3-blue" id="startAndStop" disabled>Start Video</p><br><br>
        <video id="videoInput" width=320 height=240></video>
        <canvas id="canvasOutput" width=320 height=240></canvas><br>
      </div>
      <p class="err"  id="errorMessage"></p>
    </div>

  <script src="files/utils.js" type="text/javascript"></script>

  <script type="text/javascript">
    let utils = new Utils('errorMessage');

    let streaming = false;

    let videoInput = document.getElementById('videoInput');
    let canvasOutput = document.getElementById('canvasOutput');
    let startAndStop = document.getElementById('startAndStop');
    let canvasContext = canvasOutput.getContext('2d');

    utils.loadOpenCV
    (

        () => 
        {
            utils.createFileFromUrl
            (
                'haarcascade_frontalface_default.xml', 
                'files/haarcascade_frontalface_default.xml', 
                () => 
                {
                    startAndStop.disabled = false;
                }
            );
        }
    );



    startAndStop.addEventListener('click', () => {
        if (!streaming)
        {
            utils.clearError();
            utils.startCamera('qvga', onVideoStarted, 'videoInput');
        } else {
            utils.stopCamera();
            onVideoStopped();
        }
    });



    function main()
    {
        
        let video = document.getElementById('videoInput');
        let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        let dst = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        let gray = new cv.Mat();
        let cap = new cv.VideoCapture(video);
        let faces = new cv.RectVector();
        let classifier = new cv.CascadeClassifier();
        const FPS = 30;
        let color = new cv.Scalar(255, 255, 255, 255);
        var counter = 0; 
        function processVideo()
        {
            if (!streaming)
            {
                // clean and stop.
                counter = 0; 
                src.delete();
                dst.delete();
                gray.delete();
                faces.delete();
                classifier.delete();
                return;
            }

            let begin = Date.now();
            cap.read(src);
            src.copyTo(dst);
            cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);
            classifier.detectMultiScale(gray, faces, 1.1, 3, 0);
            counter = (counter + 1) % 10;
            console.log(counter)
            for (let i = 0; i < faces.size(); ++i)
            {
                let face = faces.get(i);
                let point1 = new cv.Point(face.x, face.y);
                let point2 = new cv.Point(face.x + face.width, face.y + face.height);
                cv.rectangle(dst, point1, point2, [20, 150, 255, 255], 2);
            }

            cv.imshow('canvasOutput', dst);

            let delay = 1000/FPS - (Date.now() - begin);
            setTimeout(processVideo, delay);
        };

        classifier.load('haarcascade_frontalface_default.xml');

        setTimeout(processVideo, 0);
    }

    function onVideoStarted()
    {
        streaming = true;
        startAndStop.innerText = 'Stop Video';
        videoInput.width = videoInput.videoWidth;
        videoInput.height = videoInput.videoHeight;
        main();
    }

    function onVideoStopped()
    {
        streaming = false;
        canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
        startAndStop.innerText = 'Start Video';
        
        
    }


</script>


</body>
</html>
